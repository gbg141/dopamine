\documentclass[12pt,a4paper,openright,twoside]{article}
\usepackage{epsfig}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\parskip=5pt
\parindent=15pt
\usepackage[margin=0.85in]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{verbatim} 
\usepackage[algoruled,noline,longend]{algorithm2e}
\usepackage[colorinlistoftodos]{todonotes}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\R}{\mathbb{R}}
\DeclareMathOperator*{\N}{\mathbb{N}}
\DeclareMathOperator*{\Sspace}{\mathcal{S}}
\DeclareMathOperator*{\A}{\mathcal{A}}
\DeclareMathOperator*{\Y}{\hat{\mathcal{Y}}_{\Hat{\gamma}}}

\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{prop*}{Proposition}
\newtheorem*{corol*}{Corollary}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}

\theoremstyle{remark}
\newtheorem*{sol}{Resolution}

\theoremstyle{plain}
\newtheorem{mixtureProperty}{Property}

\def\qed{\hfill $\square$}

\graphicspath{ {./logos/} }

% --------------------------------------------------
\usepackage{fancyhdr}
\usepackage{emptypage}

\fancyhead[RO]{\small{\rightmark}}
\fancyhead[LE]{\small{\nouppercase{\leftmark}}}
\fancyhead[RE]{\small{Master Thesis}}
%\fancyhead[RE]{\small{Trabajo final de grado} \quad \quad \quad  \begin{picture}(0,0) \put(-32.84807,-3){\includegraphics[width=13mm]{./ub}} \end{picture} } 
\fancyhead[LO]{ \begin{picture}(0,0) \put(0,-3){\includegraphics[width=8mm]{./upc}} \end{picture} \quad \quad \begin{picture}(0,0) \put(0,-3){\includegraphics[width=13mm]{./ub}} \end{picture} \quad \quad \quad \begin{picture}(0,0) \put(0,-3){\includegraphics[width=9mm]{./urv}} \end{picture} \quad \quad  \quad\small{MAI} }
%\fancyhead[LO]{\begin{picture}(0,0) \put(0,-3){\includegraphics[width=13mm]{./ub}} \end{picture} \quad \quad \quad \small{Universitat de Barcelona} }
%\fancyhead[CE,CO]{\begin{picture}(0,0) \put(-17.01,-4.2){\includegraphics[width=12mm]{./ub}} \end{picture}} %foto de la ub en medio del encabezado
\fancyfoot[CO,CE]{}
\fancyfoot[RO,LE]{\thepage}

\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[RO,LE]{\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{hyperref}

%--------------------------------------------------

\begin{document}


\thispagestyle{empty}

Title


\newpage
\pagenumbering{roman} 

\newpage 


\section*{Aknowledgements}

\newpage


\section*{Abstract}

Temporal difference learning algorithms classically learn value functions (sum of discounted rewards) by solving for the fixed point that relates the value of one state with the expected value of the following states. Other TD learning algorithms have been used to learn \textit{multiplicative value functions}, which also have a fixed point solution. An example of such a function is the ratio between stationary distributions of two policies. It has been shown that learning of these multiplicative value functions suffers from higher variance and looser convergence guarantees. One potential approach to solving this issues would be to learn in log space, turning the multiplicative value function into an additive one. Unfortunately, it is easy to show with Jensen's inequality that the exponential of the log fixed point does not correspond to the multiplicative fixed point.

Distributional RL has been used to learn the return distribution (return is the sum of future discounted rewards). Although it has been shown that it mainly plays an auxiliary task role for representation learning, we propose that distributional fixed points could play a much more fundamental role to learning non additive value functions. In the case of multiplicative value functions, learning the additive fixed point in log space and exponentiating leads to the correct solution.

In particular, we will study this approach in the case of the learning process of the Covariate Shift ratio, which defines a multiplicative value function.





\newpage

%\pagenumbering{roman} 
\tableofcontents

\newpage


\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markboth{\thesection.\quad #1}{}}
\renewcommand{\subsectionmark}[1]{\markright{\thesubsection.\quad #1}}

\pagenumbering{arabic} 
\setcounter{page}{1}

\thispagestyle{plain}
\section{Introduction}

- Reinforcement Learning

- Distributional Reinforcement Learning

- Off-Policy Learning

- Covariate Shift Ratio


%\subsection*{Project}


\subsection*{Motivation and Goals}

- Relevance of logarithmic Distributional RL

- Dessign of Distributional Covariate Shift Ratio

- Evaluation of the logarithmich approach within the Distributional CSR setting


\subsection*{Structure of the Report}




\newpage


\thispagestyle{plain}
\section{Distributional Reinforcement Learning}

TODO: INTRO

Initial paper\cite{DRL}:
In this paper we argue for the fundamental impor- tance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the com- mon approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of liter- ature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy eval- uation and control settings, exposing a signifi- cant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equa- tion to the learning of approximate value distri- butions. We evaluate our algorithm using the suite of games from the Arcade Learning En- vironment. We obtain both state-of-the-art re- sults and anecdotal evidence demonstrating the importance of the value distribution in approxi- mate reinforcement learning. Finally, we com- bine theoretical and empirical evidence to high- light the ways in which the value distribution im- pacts learning in the approximate setting.

Although the distributional perspective is almost as old as Bellman’s equation itself (Jaquette, 1973; Sobel, 1982; White, 1988), in reinforcement learning it has thus far been subordinated to specific purposes: to model parametric un- certainty (Dearden et al., 1998), to design risk-sensitive al- gorithms (Morimura et al., 2010b;a), or for theoretical anal- ysis (Azar et al., 2012; Lattimore  Hutter, 2012). By con- trast, we believe the value distribution has a central role to play in reinforcement learning.

Contraction of the policy evaluation Bellman operator. Basing ourselves on results by Ro sler (1992) we show that, for a fixed policy, the Bellman operator over value distribu- tions is a contraction in a maximal form of the Wasserstein (also called Kantorovich or Mallows) metric. Our partic- ular choice of metric matters: the same operator is not a contraction in total variation, Kullback-Leibler divergence, or Kolmogorov distance.

Instability in the control setting. We will demonstrate an instability in the distributional version of Bellman’s opti- mality equation, in contrast to the policy evaluation case. Specifically, although the optimality operator is a contrac- tion in expected value (matching the usual optimality re- sult), it is not a contraction in any metric over distributions. These results provide evidence in favour of learning algo- rithms that model the effects of nonstationary policies.

Better approximations. From an algorithmic standpoint, there are many benefits to learning an approximate distribu- tion rather than its approximate expectation. The distribu- tional Bellman operator preserves multimodality in value distributions, which we believe leads to more stable learn- ing. Approximating the full distribution also mitigates the effects of learning from a nonstationary policy. As a whole, we argue that this approach makes approximate reinforce- ment learning significantly better behaved.

From a supervised learning perspective, learning the full value distribution might seem obvious: why restrict our- selves to the mean? The main distinction, of course, is that in our setting there are no given targets. Instead, we use Bellman’s equation to make the learning process tractable; we must, as Sutton and Barto (1998) put it, “learn a guess from a guess”. It is our belief that this guesswork ultimately carries more benefits than costs.

WASSERSTEIN METRIC! THEORY DEVELOPED + The Cramer Distance as a Solution to Biased Wasserstein Gradients

INTERESTING APPENDIX ON RELATED WORK!!

It is surprising that, when we use a policy which aims to maximize expected return, we should see any difference in performance. The distinction we wish to make is that learning distributions matters in the presence of approxi- mation. We now outline some possible reasons.

Reduced chattering. Our results from Section 3.4 high- lighted a significant instability in the Bellman optimal- ity operator. When combined with function approxima- tion, this instability may prevent the policy from converg- ing, what Gordon (1995) called chattering. We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distributions, similar to conservative policy iteration (Kakade AND Langford, 2002). While the chattering persists, it is inte- grated to the approximate solution.

State aliasing. Even in a deterministic environment, state aliasing may result in effective stochasticity. McCallum (1995), for example, showed the importance of coupling representation learning with policy learning in partially ob- servable domains. We saw an example of state aliasing in PONG, where the agent could not exactly predict the re- ward timing. Again, by explicitly modelling the resulting distribution we provide a more stable learning target.

A richer set of predictions. A recurring theme in artificial intelligence is the idea of an agent learning from a mul- titude of predictions (Caruana 1997; Utgoff and Stracuzzi 2002; Sutton et al. 2011; Jaderberg et al. 2017). The dis- tributional approach naturally provides us with a rich set of auxiliary predictions, namely: the probability that the return will take on a particular value. Unlike previously proposed approaches, however, the accuracy of these pre- dictions is tightly coupled with the agent’s performance.

Framework for inductive bias. The distributional perspective on reinforcement learning allows a more natural framework within which we can impose assumptions about the domain or the learning problem itself. In this work we used distributions with support bounded in [VMIN,VMAX]. Treating this support as a hyperparameter allows us to change the optimization problem by treating all extremal returns (e.g. greater than VMAX ) as equivalent. Surprisingly, a similar value clipping in DQN significantly degrades per- formance in most games. To take another example: in- terpreting the discount factor as a proper probability, as some authors have argued, leads to a different algorithm.


Well-behaved optimization. It is well-accepted that the KL divergence between categorical distributions is a rea- sonably easy loss to minimize. This may explain some of our empirical performance. Yet early experiments with al- ternative losses, such as KL divergence between continu- ous densities, were not fruitful, in part because the KL di- vergence is insensitive to the values of its outcomes. A closer minimization of the Wasserstein metric should yield even better results than what we presented here.

An Analysis of Categorical Distributional Reinforcement Learning

Distributional Reinforcement Learning with Quantile Regression

Implicit Quantile Networks for Distributional Reinforcement Learning

DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS

A Comparative Analysis of Expected and Distributional Reinforcement Learning

Statistics and Samples in Distributional Reinforcement Learning:




\subsection{Reinforcement Learning Setting}

We consider an agent interacting with an environment in the standard setting\cite{rlformulation}: at each step $t$, the agent selects an action $a_t$ based on its current state $s_t$, to which the environment responds with a reward $r_t$ and then moves to the next state $s_{t+1}$. We model this interaction as a time-homogeneous Markov Decision Process $(\Sspace, \A, r, P, \gamma)$, where
\begin{itemize}
    \item $\Sspace$ and $\A$ are the state and action spaces, respectively, we assume that both are finite, with $n:=|\Sspace|$;
    \item $P$ is the transition kernel, $s_{t+1} \sim P(\cdot | s_t,a_t)$; the Markov assumption states that \\ $P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)$;
    \item $r(s,a)$ represents the immediate reward given by the environment after taking action $a$ being in state $s$. These rewards are considered to be sampled from the reward function $R(s,a)$, i.e. $r_t \sim R(s_t,a_t)$;
    \item $\gamma$ is the discount factor
\end{itemize}
A policy $\pi$ maps each state to a probability distribution over the action space, $a_t \sim \pi(\cdot | s_t)$. In addition, we combine the policy $\pi$ and transition function $P$ into a state-to-state transition function $P_\pi \in \R^{n \times n}$ , whose entries are
\begin{equation} \label{transitionFunction}
    P_{\pi} (s'|s) := Prob_\pi (s_{t+1}=s' | s_t = s) = \sum_{a\in\A} \pi(a|s)P(s'|s,a)
\end{equation}
In particular, powers of $P_\pi$ represent the transition function across different time-steps.

Given $\pi$, the action value function is defined as the expected sum of discounted rewards from a state-action pair by following the policy:
\begin{equation} \label{AV}
Q^\pi (s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t,a_t) \bigg| s_0=s, a_0=a \right] 
\end{equation}
The Bellman's equation can be obtained from this expression:
\begin{equation} \label{BE}
\begin{split}
    Q^\pi (s,a) &= \mathbb{E} \left[ r(s,a) \right] + \gamma \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_0=s, a_0=a \right] \\
    &= \mathbb{E} \left[ r(s,a) \right] + \gamma \sum_{s'} P(s'|s,a) \left( \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_1=s' \right] \right) \\
    &= \mathbb{E} \left[ r(s,a) \right] + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s')\left( \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_1=s', a_1=a' \right] \right)\\
    %&= \mathbb{E} \left[ r(s,a) \right] + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s') Q(s',a') \\
    &= \mathbb{E} \left[ r(s,a) \right] + \gamma \E_{s'\sim P(\cdot | s,a), a' \sim \pi(\cdot | s')} \bigg[Q(s',a') \bigg] \\
\end{split}
\end{equation}

Analogously for the state value function (considering $r_\pi(s) := \mathbb{E}_{a\sim\pi(\cdot | s)} \left[ r(s,a) \right]$):
\begin{equation} \label{SV}
\begin{split}
    V^\pi (s) &= \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t,a_t) \bigg| s_0=s\right] \\
    &= \mathbb{E}_{a\sim\pi(\cdot | s)} \left[ r(s,a) \right] + \gamma \sum_a \pi(a | s) \sum_{s'} P(s'|s,a) \left( \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_1=s' \right] \right) \\
    &= r_\pi(s) + \gamma \E_{s'\sim P_{\pi}(\cdot | s)} \bigg[V^\pi(s') \bigg]
\end{split}
\end{equation}



\subsection{Notation Analysis of Distributional Reinforcement Learning}

TODO: INTRO

In \cite{DRL} a distributional Bellman equation is defined:
\begin{equation} \label{distBE}
    Z^{\pi}(s,a) \stackrel{D}{=} R(s,a) + \gamma Z^{\pi}(S',A')
\end{equation}
where
\begin{itemize}
    \item $Z^\pi$ is the \textit{random return} from a state-action pair by following policy $\pi$, whose expectation is the value $Q^\pi$
    	\begin{equation} \label{QexpZ}
        		Q^\pi(s,a) := \mathbb{E} [Z^\pi(s,a)]
    	\end{equation}
    	It is also called the value distribution.
    \item $(S',A')$ is the next state-action random variable: $s'\sim S'$ with  $P(s' | s,a)$, $A' \sim \pi(\cdot | S')$
    \item $R(s,a)$ is the random reward, or equivalently the reward function. Note that now we are dealing with it as an explicit random variable.
    \item $Z^\pi(S',A')$ is the random return over the random next state-action following $\pi$. This notation implies that all possible next state-action pairs need to be considered as to generate this return distribution. Thus, $Z^\pi(S',A')$ may be seen as a mixture distribution of the distributions $Z^\pi(s',a')$ where $s'$ and $a'$ are sampled from $(S',A')$:
    \begin{equation}
        f_{Z^\pi(S',A')} (z) = \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{Z^\pi(s',a')}(z)
    \end{equation}
    The expected value, using \ref{QexpZ}, can be then expressed as:
    \begin{equation} \label{expNextRandomReturn}
    \begin{split}
        \E[Z^\pi(S',A')] &=  \int_{-\infty}^{\infty} z \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{Z^\pi(s',a')}(z) dz \\
        &=  \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') \int_{-\infty}^{\infty} z f_{Z^\pi(s',a')}(z) dz \\
        &=  \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') \E[Z^\pi(s',a')] \\
        &= \E_{s'\sim P(\cdot | s,a), a' \sim \pi(\cdot | s')} \bigg[Q^\pi(s',a') \bigg] 
    \end{split}
    \end{equation}
\end{itemize}

Note that we can easily recover the classical Bellman's equation \ref{BE} for the action value Q by using \ref{QexpZ} and \ref{expNextRandomReturn} when taking the expected value over its distributional version \ref{distBE}:
\begin{equation}
\begin{split}
    Q^\pi(s,a) &= \mathbb{E} [Z^\pi(s,a)] \\
    &= \E[R(s,a)] + \gamma \E[Z^\pi(S',A')] \\
    &= \mathbb{E} [r(s,a)] + \gamma \E_{s'\sim P(\cdot | s,a), a' \sim \pi(\cdot | s')} \bigg[Q^\pi(s',a') \bigg] 
\end{split}
\end{equation}

Finally, let's try to find out what actually the random return $Z$ represents by expanding its density function: 
\begin{equation} \label{meaningZ}
\begin{split}
    f_{Z^\pi(s,a)}(z) &= f_{R(s,a) + \gamma Z^\pi(S',A')}(z)  \\
    &=  \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{R(s,a) + \gamma Z^\pi(s',a')}(z) \\
    &= \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{R(s,a) + \gamma (R(s',a') + \gamma Z^\pi(S'',A''))}(z) \\
    &= \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') \sum_{s''} P(s'' | s',a') \sum_{a''} \pi(a''|s'') \\
    & \quad \quad f_{R(s,a) + \gamma R(s',a') + \gamma^2 Z^\pi(s'',a'')} (z) \\
    &= \sum_{s_{1}} P(s_{1} | s_{0},a_{0}) \sum_{a_{1}} \pi(a_{1}|s_{1})  \cdots \sum_{s_{t}} P(s_{t} | s_{t-1},a_{t-1}) \sum_{a_{t}} \pi(a_{t}|s_{t}) \\
    & \quad \quad f_{\sum_{i=0}^{t} \gamma^t R(s_{t},a_{t})} (z)
\end{split}
\end{equation}
where we have repeatedly used property \ref{mixtureProperty2} of mixture distributions. In addition, note that assuming independence between the random reward $R$ of a certain state-action pair and the return distribution $Z^\pi$ of the possible next state-action (which is NOT TRUE in general), we can rewrite it in terms of convolutions as
\begin{equation}
\begin{split}
    f_{Z^\pi(s,a)}(z) &= \sum_{s_{1}} P(s_{1} | s_{0},a_{0}) \sum_{a_{1}} \pi(a_{1}|s_{1})  \cdots \sum_{s_{t}} P(s_{t} | s_{t-1},a_{t-1}) \sum_{a_{t}} \pi(a_{t}|s_{t}) \\
    & \quad \quad  \left( f_{R(s_0,a_0)} * f_{\gamma R(s_1,a_1)} *  \cdots * f_{\gamma^t R(s_{t},a_{t})} \right) (z)
\end{split}
\end{equation}

According to \ref{meaningZ}, $Z(s,a)$ can be interpreted as a convex combination of the sum of discounted reward distributions of all possible agent trajectories starting at the state-action $(a,s)$ and following policy $\pi$ from then on, each weight corresponding to the probability of that precise trajectory. 

\newpage


\thispagestyle{plain}

\thispagestyle{plain}
\section{Covariate Shift Ratio}

TODO: INTRO off-policy learning and COPTD

Original paper \cite{COPTD}:

The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(lambda, beta)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD(lambda, beta) can be designed to converge to the same value that would have been obtained by using on-policy TD(lambda) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call log COP-TD(lambda, beta). Both algorithms have favorable empirical results to the current state of the art online OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.

Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013). We consider the problem of offpolicy evaluation (OPE) – assessing the performance of a complex strategy without applying it. An OPE formulation is often considered in domains with limited sampling capability. For example, marketing and recommender systems (Theocharous  Hallak, 2013; Theocharous et al., 2015) directly relate policies to revenue. A more extreme example is drug administration, as there are only few patients in the testing population, and sub-optimal policies can have life threatening effects (Hochberg et al., 2016). OPE can also be useful as a module for policy optimization in a policy improvement scheme (Thomas et al., 2015a).In this paper, we consider the OPE problem in an on-line setup where each new sample is immediately used to update our current value estimate of some previously unseen policy. We propose and analyze a new algorithm called COP-TD.

Our algorithm resembles \cite{ETD} Emphatic TD that was extended by \cite{generalizedETD} to the general parametric form ETD(lambda, beta). We clarify the connection between the algorithms and compare them empirically. Unlike ETD(lambda, beta), COP-TD(lambda,beta)’s effectiveness depends on the available resources. The number of features can be adjusted accordingly to provide the most affordable approximation. The added cost is fine-tuning another step- size, though beta’s effect is less prominent.


Conclusions: Research on off-policy evaluation has flourished in the last decade. While a plethora of algorithms were suggested so far, ETD(lambda, beta) by Hallak et al. (2015) has perhaps the simplest formulation and theoretical properties. Unfortunately, ETD(lambda, beta) does not converge to the same point achieved by on-line TD when linear function approximation is applied.We address this issue with COP-TD(lambda,beta) and proved it can achieve consistency when used with a correct set of features, or at least allow trading-off some of the bias by adding or removing features. Despite requiring a new set of features and calibrating an additional update function, COP-TD(lambda,beta)’s performance does not depend as much on beta as ETD( lambda, beta), and shows promising empirical results.

Paper Carles Discounted COP-TD \cite{DCOPTD}:

Central to reinforcement learning is the idea that an agent should learn from experience. While many algorithms learn in a purely online fashion, sample-efficient methods typically make use of past data, viewed either as a fixed dataset, or stored in a replay memory (Lin 1993, Mnih et al. 2015). Because this past data may not be generated according to the policy currently under evaluation, the agent is said to be learning off-policy (Sutton and Barto 2018).

By now it is well-documented that off-policy learning may carry a signifcant cost when combined to function approximation. Early results have shown that estimating the value function off-policy, using Bellman updates, may diverge (Baird 1995), (Tsitsiklis and Van Roy 1997). More recently, value divergence was perhaps the most significant issue dealt with in the design of the DQN agent (Mnih et al. 2015), and remains a source of concern in deep reinforcement learning (van Hasselt, Guez, and Silver 2016).

Further, under off-policy learning, the quality of the Bellman fixed point suffers as studied by Kolter (2011) and Munos (2003). The value function error can be unboundedly large even if the value function can be perfectly approximated. Hence, even in the case where convergence to the fixed point with off-policy data occurs, solutions can be of poor quality. Thus, the existing TD learning algorithms with convergence guarantees under off-policy data (Maei et al. 2009), (Sutton et al. 2009) can still suffer from off-policy issues.

This paper studies the covariate shift method for dealing with the off-policy problem. The covariate shift method, studied by Hallak and Mannor (2017) and Sutton, Mahmood, and White (2016), reweights online updates according to the ratio of the target and behavior stationary distributions. Under optimal conditions, the covariate shift method recovers convergent behavior with linear approximation, breaking what Sutton and Barto (2018) call the “deadly triad” of reinforcement learning. We argue the method is particularly appealing in the context of replay memories, where the reweighting can be replaced by a reprioritization scheme similar to that of Schaul et al. (2016).

We improve on Hallak and Mannor’s COP-TD algorithm, which has provable guarantees but is dificult to implement in a deep reinforcement learning setting. First, we introduce a discount factor into their update rule to obtain a more stable algorithm. Second, we develop an alternative normalization scheme that can be combined with deep networks, avoiding the projection step necessary in the original algorithm.

Conclusion: In this paper we revisited Hallak and Mannor’s COP-TD algorithm and extended it to be applicable to the deep reinforcement learning setting. While these results on the Atari 2600 suite of games remain preliminary, they demonstrate the practicality of learning the covariate shift in complex settings. We believe our results further open the door to increased sample efficiency in deep reinforcement learning.

We emphasize that the instabilities observed when learning the covariate shift under prioritized sampling point to the importance of the data distribution used to learn the ratios. Which distribution is optimal will be the focus of future work. The covariate shift method is a “backward” off-policy method, in the sense that it corrects a mismatch between distributions based on past transitions. It would be interesting to combine our method to “forward” off-policy methods such as Retrace (Munos et al. 2016), which have also yielded good results on the Atari 2600 (Gruslys et al. 2018). Then, it would be interesting to understand whether overfitting does occur due to a smaller effective replay size, and how this can be addressed. Finally, an exciting avenue would be extending the method to the more general case where multiple policies have generated off-policy data, which would allow COP-TD to be applied in the standard control setting.


\subsection{COP-TD}

As stated in \cite{DCOPTD}, here we move to the \textit{policy evaluation} problem within \textit{off-policy learning}, where we want to learn the value function $V^\pi$ of a \textit{target policy} $\pi$ from samples drawn from $P$ and a \textit{behaviour policy} $\mu$. Some useful notation:
\begin{itemize}
    \item The Bellman equation for the state value function can be expressed in vector notation as $V^\pi = r_\pi + \gamma P_\pi V^\pi$, where $V^\pi \in \R^{n}$, $r_\pi \in \R^n$ and $P_\pi \in \R^{n \times n}$. The value function is in fact the fixed point of the \textit{Bellman operator} $\mathcal{T}_\pi: \R^{n \times n} \rightarrow \R^{n \times n}$, defined as $\mathcal{T}_\pi V := r_\pi + \gamma P_\pi V $. It defines a single step of \textit{bootstrapping}: the process $V^{k+1} := \mathcal{T}_\pi V^k$ converges to $V^\pi$.
    \item Let $d \in \R^n$; we write $D_d \in \R^{n\times n}$ for the corresponding diagonal matrix, and consider the weighted squared seminorm notation of vectors $x\in\R^n$ $||x||^2_{A} := ||A x||^2 = x^T A^T A x$, $||x||^2_d :=||x||^2_{D_d}= \sum_{i=1}^n d(i)^2 x(i)^2$.
    \item $e\in\R^n$ accounts for the vector of all ones, and $\Delta(\Sspace)$ for the simplex over states: $d \in \Delta(\Sspace) \implies d^T e = 1, d \geq 0$.
    \item $d \in \Delta(\Sspace)$ is the stationary distribution of a transition function $P$ if and only if $d=d \cdot P$. This distribution is unique when $P$ defines a Markov chain with a single recurrent class\cite{unichain}.
\end{itemize}
In this particular setting we distinguish between two different state-to-state transition functions, $P_\pi$ and $P_\mu$, one for each policy; their respective stationary distributions will be represented by $d_\pi$ and $d_\mu$. 

It is common to estimate the value function through \textit{linear function approximation}, which uses a mapping from states to features $\phi: \Sspace \rightarrow \R^k$. In these cases, the approximate value function at a state $s$ can be expressed as the inner product of a feature vector with a vector of weights $\theta \in \R^k$:
\begin{equation} \label{linearApproxSV}
    \Hat{V}(s) = \phi(s)^T \theta
\end{equation}
which can be written as $\Hat{V} = \Phi \theta$ in vector notation, being $\Phi \in \R^{n\times k}$ the matrix of row vectors. The \textit{semi-gradient update rule} for TD learning\cite{TD} learns an estimation of $V^\pi$ from sample transitions. Given a starting state $s\in\Sspace$, a successor state $s'\sim P_\pi(\cdot | s)$, and a step-size parameter $\alpha > 0$, this update is
\begin{equation} \label{updateTD}
    \theta \leftarrow \theta + \alpha \left[ r_\pi(s) + \gamma\phi(s')^T \theta - \phi(s)^T \theta \right] \phi(s)
\end{equation}

The expected behaviour of this update rule is described by the \textit{projected Bellman operator} $\Pi_d \mathcal{T}_\pi$, a combination of the usual Bellman operator with a projection $\Pi_d$ in norm $||\cdot||_d$ -for some $d\in\Delta(\Sspace)$- onto the span of $\Phi$\cite{projectedBellman}. In fact, the stationary point of \ref{updateTD}, if it exists, is the solution of the \textit{projected Bellman equation} $\Hat{V}^\pi = \Pi_d \mathcal{T}_\pi \Hat{V}^\pi$. As stated in \cite{DCOPTD}, not only convergence is proved for $d=d_\pi$, but this choice also seems optimal in terms of the quality of the fixed point under off-policy data.

Supposing that stationary distributions $d_\pi$ and $d_\mu$ are known, and that states are updated according to $s\sim d_\mu$, the covariate shift approach presented in \cite{COPTD} uses importance sampling to redefine \ref{updateTD} so that the semi-gradient update rule can be considered \textit{under the sampling distribution} $d_\pi$:
\begin{equation} \label{updateTDimportance}
    \theta \leftarrow \theta + \alpha \frac{d_\pi(s)}{d_\mu (s)}\left[ r(s,a) + \gamma\phi(s')^T \theta - \phi(s)^T \theta \right] \phi(s)
\end{equation}
with $a\sim \mu(\cdot|s)$, $s' \sim P(\cdot|s,a)$ as before. 

Both the original COP-TD learning rule\cite{COPTD} and its discounted version\cite{DCOPTD} seek to learn the ratio $d_\pi / d_\mu$ from samples -by bootstrapping from a previous prediction, similar to temporal difference learning. For instance, given a sample transition $(s_t,a_t,s_{t+1}) = (s,a,s')$ drawn from $d_\mu$, $\mu(\cdot|s)$ and $P(\cdot|s,a)$, respectively, and for $c\in \mathbb{R}^n$, step size $\alpha>0$ and discount factor $\hat{\gamma} \in [0,1]$, the Discounted COP-TD provide us with the following update
\begin{equation} \label{DCOP}
    c(s') \leftarrow c(s') + \alpha \left[\hat{\gamma} \frac{\pi(a|s)}{\mu(a|s)}c(s) + (1-\hat{\gamma}) - c(s') \right]
\end{equation}
This update rule learns “in reverse” compared to TD learning, its expected behaviour being captured by the operator $Y_{\Hat{\gamma}}$
\begin{equation} \label{DCOPoperator}
    (Y_{\Hat{\gamma}}c)(s') := \E_{s \sim d_\mu, a \sim \mu(\cdot|s)} \left[ \hat{\gamma} \frac{\pi(a|s)}{\mu(a|s)}c(s) + (1-\hat{\gamma}) \bigg| s' \right] = \hat{\gamma} (Yc)(s') + (1-\hat{\gamma})
\end{equation}
where $Y$ is the original COP operator 
\begin{equation} \label{COPoperator}
    (Yc)(s') := \E_{s \sim d_\mu, a \sim \mu(\cdot|s)} \left[ \frac{\pi(a|s)}{\mu(a|s)}c(s)  \bigg| s' \right]
\end{equation}
which corresponds to the undiscounted case, $Y_1=Y$.

Note that the condition $s_{t+1} = s'$ in the expectation of \ref{COPoperator} forces to take into account the distribution of previous state-action pairs $(s,a)$ according to policy $\mu$. The distribution of the possible previous states $s$ is given by the time-reversal transition function $\Bar{P}_\mu$, whose entries are:
\begin{equation}
\begin{split}
    \Bar{P}_\mu (s | s') &:= Prob_\mu (s_t = s | s_{t+1} = s') \\
    &= \frac{Prob_\mu ( s_{t+1} = s' | s_t = s) Prob_\mu(s_t=s)}{Prob_\mu ( s_{t+1} = s')} \\
    &= \frac{P_\mu (s' | s) d_\mu(s)}{d_\mu(s')} 
\end{split}
\end{equation}
Or, equivalently, $ \Bar{P}_\mu = D_{d_\mu}^{-1}P_\mu^T D_{d_\mu}$ in vector notation. Regarding the distribution of the possible actions that lead to $s'$ from a certain state $s$ by following policy $\mu$, it will be represented by function $\Bar{\mu}$:  
\begin{equation}
\begin{split}
        \Bar{\mu}(a|s,s') &:= Prob_\mu(a_t=a | s_t = s, s_{t+1}=s') \\
        &= \frac{Prob_\mu (a_t=a, s_t = s, s_{t+1}=s')}{Prob_\mu (s_t = s, s_{t+1}=s')} \\
        %&= \frac{Prob_\mu (s_{t+1}=s' | a_t=a, s_t = s) Prob_\mu(a_t=a, s_t = s)}{Prob_\mu (s_t = s, s_{t+1}=s')} \\
        &= \frac{Prob_\mu (s_{t+1}=s' | a_t=a, s_t = s) Prob_\mu (a_t=a | s_t = s) Prob_\mu (s_t = s)}{Prob_\mu (s_{t+1}=s' | s_t = s) Prob_\mu (s_t = s)} \\
        &= \frac{P(s'|s,a)\mu(s|a)}{P_\mu (s'|s)}
\end{split}
\end{equation}


With the introduced notation, the expectation in \ref{COPoperator} can be rewritten and expanded in the following way:
\begin{equation} \label{expansionCSR}
\begin{split}
    \E_{s \sim d_\mu, a \sim \mu(\cdot|s)} \left[ \frac{\pi(a|s)}{\mu(a|s)}c(s)  \bigg| s' \right] &= \E_{s \sim \Bar{P}_\mu (\cdot | s'), a \sim \Bar{\mu}(\cdot|s,s')} \left[ \frac{\pi(a|s)}{\mu(a|s)}c(s) \right] \\
    &= \sum_s \Bar{P}_\mu (s | s') \sum_a \Bar{\mu}(a|s,s') \frac{\pi(a|s)}{\mu(a|s)} c(s) \\
    &= \sum_s \left( \frac{P_\mu (s' | s) d_\mu(s)}{d_\mu(s')}  \right) \sum_a  \left( \frac{P(s'|s,a)\mu(s|a)}{P_\mu (s'|s)} \right) \frac{\pi(a|s)}{\mu(a|s)} c(s) \\
    &= \frac{1}{d_\mu(s')} \sum_{s} d_\mu(s) c(s) \sum_a \pi(a|s) P(s'|s,a) \\
    &= \frac{1}{d_\mu(s')} \sum_{s} P_\pi(s'|s) d_\mu(s) c(s) \\
    %&=  \frac{1}{d_\mu(s')} \E_{s\sim d_\mu} \left[P_\pi(s'|s) c(s)\right]
\end{split}
\end{equation}
Thus, the COP and DCOP operators can be expressed in vector notation, respectively, as
\begin{equation}
    Yc = D_{d_\mu}^{-1}P_\pi^T D_{d_\mu} c
\end{equation}

\begin{equation}
    Y_{\Hat{\gamma}}c = \Hat{\gamma} D_{d_\mu}^{-1}P_\pi^T D_{d_\mu} c + (1-\Hat{\gamma})e
\end{equation}

\newpage

\thispagestyle{plain}
\section{Distributional COP-TD}

TODO: INTRO

Now we are interested in going beyond the notion of value and consider the estimation of the CS ratio from a distributional perspective, similarly to what was done in \cite{DRL} within the reinforcement learning setting. Our starting point could be 
\begin{equation} \label{distCSR}
    X(s') \stackrel{D}{=} \frac{\pi(A^\mu_{s,s'}|S^\mu_{s'})}{\mu(A^\mu_{s,s'}|S^\mu_{s'})} X(S^\mu_{s'}) 
    %X(s') \stackrel{D}{=} \hat{\gamma} \frac{\pi(A^\mu_{s,s'}|S^\mu_{s'})}{\mu(A^\mu_{s,s'}|S^\mu_{s'})} X(S^\mu_{s'}) +1 -\hat{\gamma}
\end{equation}
where
\begin{itemize}
    \item $X$ is the random ratio between distributions of achieving a certain state, its expectation being the covariate shift ratio 
    \begin{equation}
        \frac{d_\pi}{d_\mu}(s) = c(s) = \E [X(s)]
    \end{equation}
    
    \item $(S^\mu_{s'},A^\mu_{s,s'} )$ is the previous state-action random variable:
   	\begin{itemize}
    	\item The random variable $S^\mu_{s'}$ represents the states from which state $s'$ is achievable by following policy $\mu$; $S^\mu_{s'} =s$ with probability $\Bar{P}_\mu(s|s')$
   	 	\item $A^\mu_{s,s'}$ encodes the random action that can be taken to get state $s'$ from a state $s\sim S^\mu_{s'}$ according to policy $\mu$, so $A^\mu_{s,s'} =a$ with probability $\Bar{\mu}(a|S^\mu_{s'},s')$
   	 \end{itemize}
 
\end{itemize}

Recalling equation \ref{distCSR}, note that it intrinsically expresses the random ratio of a state $s_{t+1}$, $X(s_{t+1})$, as a mixture distribution with the 'corrected' previous state random ratios $\rho(s_{t},a_{t}) X(s_{t})$ as mixing components, and the previous state-action random variable $(S^\mu_{s_{t+1}},A^\mu_{s_{t},s_{t+1}} )$ as the mixing distribution:
\begin{equation} \label{densityDCS}
    f_{X(s_{t+1})}(x) = \sum_{s_{t}} \Bar{P}_\mu (s_{t} | s_{t+1}) \sum_{a_t} \Bar{\mu}(a_t|s_{t},s_{t+1}) f_{\frac{\pi(s_{t},a_{t})}{\mu(s_{t},a_{t})} X(s_{t})}(x) 
\end{equation}

\begin{notation}
	So as to reduce the complexity and increase the readability of the formulation, we introduce the following notation:
	
	\begin{itemize}
		\item Let define $\rho$ the policy ratio $\pi / \mu$:
%\begin{equation}
%	\rho(s_{t},a_{t}) = \rho (A^\mu_{s_{t},s_{t+1}},S^\mu_{s_{t+1}}) := \frac{\pi(A^\mu_{s_{t},s_{t+1}}|S^\mu_{s_{t+1}})}{\mu(A^\mu_{s_{t},s_{t+1}}|S^\mu_{s_{t+1}})}
%\end{equation}
		\begin{equation*}
			\rho(a,s) := \frac{\pi(a|s)}{\mu(a|s)}
		\end{equation*}

	\item Note in equation \ref{densityDCS} that there are as many mixture components as state-action pairs $(s_t,a_t)$; thus, we can iterate the summation over all these possible pairs and define each corresponding mixture weight $\alpha$ as
		\begin{equation*}
			\alpha(s_{t},a_{t}; s_{t+1}) = \Bar{P}_\mu (s_{t} | s_{t+1}) \Bar{\mu}(a_t|s_{t},s_{t+1}) 
		\end{equation*}
	\end{itemize}
\end{notation}

Considering the previous notation, the expression of the density function \ref{densityDCS} can be rewritten and expanded in the following way:
\begin{equation}
\begin{split}
    f_{X(s_{t+1})}(x) &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\rho(s_{t},a_{t}) X(s_{t})}(x) \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\rho(s_{t},a_{t}) \rho(S^{\mu}_{s_{t}},A^{\mu}_{s_{t-1},s_{t}}) X(S^{\mu}_{s_t}) )}(x) \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \sum_{(s_{t-1},a_{t-1})} \alpha(s_{t-1},a_{t-1};s_{t})   \cdot f_{\rho(s_{t},a_{t}) \rho(s_{t-1},a_{t-1}) X(s_{t-1})}(x) \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdots \sum_{(s_{0},a_{0})} \alpha(s_{0},a_{0};s_{1}) \cdot f_{ \left(\prod_{i=t}^{0} \rho(s_{i},a_{i})\right)  X(s_{0})}(x)
\end{split}
\end{equation}

Regarding its expectation, we can see that:
\begin{equation}
\begin{split}
    \E[X (s_{t+1})] &= \int_{-\infty}^{\infty} x f_{X(s_{t+1})}(x) dx \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \int_{-\infty}^{\infty} x f_{\rho(s_{t},a_{t}) X(s_{t})}(x) dx \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \E \left[ \rho(s_{t},a_{t}) X(s_{t}) \right] \\
    &=  \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \rho(s_{t},a_{t}) \cdot \E \left[X(s_{t}) \right] 
\end{split}
\end{equation}

\subsubsection*{Discounted Distributional CS Ratio}
Having already develop the previous formulation, it is straightforward to turn equation \ref{distCSR} into its discounted version as it is done in \cite{DCOPTD} in the value-based case:
\begin{equation} \label{distDCSR}
    X(s_{t+1}) \stackrel{D}{=} \hat{\gamma} \ \rho(S^{\mu}_{s_{t+1}},A^{\mu}_{s_{t},s_{t+1}}) X(S^{\mu}_{s_{t+1}}) + 1 -\hat{\gamma}
\end{equation}
as well as compute its corresponding expectation:
\begin{equation}
\E[X (s_{t+1})] = 1- \hat{\gamma}\left(1 - \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \rho(s_{t},a_{t}) \cdot \E \left[X(s_{t}) \right]  \right)
\end{equation}

Given that $\E \left[X(s) \right] = c(s)$, the previous equation is equivalent to the result of applying of the DCOP operator, whose convergence is analyzed and proved in \cite{DCOPTD}. This can help us prove the convergence to a fixed point in the distributional setting, whose expectation matches with the covariate shift ratio estimate $c$ in the value-based case.


\newpage

\thispagestyle{plain}
\section{Logarithmic Approach to Distributional COP-TD}

Note that the Distributional Covariate Shift equation is purely multiplicative, and so it is the associated update rule in the learning setting. 


Let's consider
\begin{equation}
	Y(s_{t+1}) := \log \left( X(s_{t+1}) \right) = \log \left( \rho(S^{\mu}_{s_{t+1}},A^{\mu}_{s_{t},s_{t+1}}) \right) + Y(S^{\mu}_{s_{t+1}}) 
\end{equation}
Its density function being
\begin{equation}
\begin{split}
	f_{Y(s_{t+1})} (x) &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\log\left( \rho(s_{t},a_{t}) X(s_{t}) \right)}(x) \\
	&= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\log\left( \rho(s_{t},a_{t})\right) + Y(s_t)}(x)
\end{split}
\end{equation}

Recalling \textbf{Property \ref{mixtureProperty3}} of Mixture Distributions, we can express the expectation of $Y$ as:
\begin{equation}
\begin{split}
	\E[Y(s_{t+1})] &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \E \left[ \log \left( \rho(s_{t},a_{t}) X(s_{t}) \right) \right] \\
	&= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \E \left[ \log \left( \rho(s_{t},a_{t}) \right)  + Y(s_t) \right] \\
	&= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \left( \log \left( \rho(s_{t},a_{t}) \right) + \E [Y(s_t)] \right)
\end{split}
\end{equation}

TODO: ADDITIVE FIXED POINT
\\

And if
\begin{equation}
	U(s') = \exp (Y(s'))
\end{equation}
expectation:
\begin{equation}
\begin{split}
	\E[U(s_{t+1})] &= \int_{-\infty}^{\infty} \exp(y) f_{Y(s_{t+1})} (y) dy \\
	&= \int_{-\infty}^{\infty} \exp(\log(x)) f_{X(s_{t+1})} (x) dx \\
	&=  \int_{-\infty}^{\infty} x f_{X(s_{t+1})} (x) dx  \\
	&= \E [X(s_{t+1})]
\end{split}
\end{equation}


\newpage

\thispagestyle{plain}
\section{Implementation}

\subsection{Categorical COP-TD}

Analogously to \cite{DRL}, we can attempt to model the CS ratio distribution through a discrete parametric distribution with a certain set of atoms $\{x_i\}_{0 \leq i<M}$, $M \in \mathbb{N}$, as its support. The atom probabilities would be given by a parametric model $\theta : \mathcal{X} \rightarrow \mathbb{R}^M$
\begin{equation}
    X_{\theta} (s) = x_i \quad w.p. \quad p_i(s) := f_{\theta}^i(s)
\end{equation}

The DCOP update $Y_{\hat{\gamma}}X_\theta$ and this parametrization $X_\theta$ almost always would have disjoint supports. This could be tackled in practice projecting the sample DCOP update $\hat{Y}_{\hat{\gamma}}X_\theta$ onto the support of $X_\theta$ (i.e. given a sample transition $(s,a,s')$, we compute the DCOP update $\hat{Y}_{\hat{\gamma}}x_j =  \frac{\pi(a|s)}{\mu(a|s)}x_j + (1-\hat{\gamma})$ for each atom $x_j$, then distribute its probability $p_j(s)$ to the immediate neighbours of $\hat{Y}_{\hat{\gamma}}x_j$).

The corresponding pseudocode is detailed in Algorithm \ref{distCSalgorithm}, analogous to that of \cite{DRL}:

\setlength{\algomargin}{1.5em}
\SetAlCapHSkip{0em}
\begin{algorithm}[H]
\caption{Categorical CS Algorithm} \label{distCSalgorithm}
\DontPrintSemicolon
\SetKwComment{CustomComment}{\#}{}
\SetKwInput{KwIn}{\hspace{-1.5em} input}
\SetKwInput{KwOut}{\hspace{-1.5em} output}
\KwIn{A transition $s_{t-1},a_{t-1},s_t$}
    %$\frac{d_\pi}{d_\mu}(s_{t-1}) := \sum_i x_i p_i(s_{t-1})$\;
    $m_i = 0$, $i\in \{0,\dots, M-1\}$\;
    \For{$j\in \{0,\dots, M-1\}$}{
        \CustomComment{Compute the projection $\tilde{T}x_j$ onto the support $\{x_i\}$}
        $\tilde{T}x_j \leftarrow \left[ \hat{\gamma} \frac{\pi(a_{t-1}|s_{t-1})}{\mu(a_{t-1}|s_{t-1})} x_j + 1 - \hat{\gamma} \right]_{C_{min}}^{C_{max}}$\;
        $b_j \leftarrow (\tilde{T}x_j -C_{min}) / \Delta x \quad$ \CustomComment{note that $b_j \in [0, M-1]$}
        $l \leftarrow \lfloor b_j \rfloor$, $u \leftarrow \lceil b_j \rceil$\;
        \CustomComment{Distribute probability of $\tilde{T}x_j$}
        $m_l \leftarrow m_l + p_j(s_{t-1}) (u - b_j)$\;
        $m_u \leftarrow m_u + p_j(s_{t-1}) (b_j - l)$\;
    }
\KwOut{Cross-entropy loss $-\sum_i m_i \log(p_i(s_t))$}

\end{algorithm}

Observations:
\begin{itemize}
    \item $C_{min}, C_{max} \in \R$ are the predefined lower and upper value limits of the covariate shift ratio. In this case, $C_{min} \geq 0$.
    \item The support is evenly spaced in $[C_{min}, C_{max}]$; we consider the set of atoms $\{x_i = C_{min} + i \Delta x : 0 \leq i < M \}$, with $\Delta x = \frac{C_{max}-C_{min}}{M-1}$
    \item The \textit{behavioural policy} $\mu$ is simply the uniformly random policy, so 
    \begin{equation} \label{behaviourPolicy}
        \mu(a|s) = \frac{1}{|\mathcal{A}|} \quad \forall a \in \mathcal{A}
    \end{equation}
    for any state $s \in \mathcal{S}$.
    \item The \textit{target policy} $\pi$ is the $\epsilon$-greedy policy with respect to the estimated state-action values of the model. Hence,%expected value of the distribution output of the target network. Hence,
    \begin{equation} \label{targetPolicy}
        \pi_\theta (a|s) = 
        \left\{ 
            \begin{array}{lcc}
                (1-\epsilon)+\epsilon \frac{1}{|\mathcal{A}|} &   \text{if} & a = \text{arg max}_a Q_\theta(s,a) \\
                \epsilon \frac{1}{|\mathcal{A}|} &  \text{otherwise} &
            \end{array}
        \right. 
    \end{equation}
   for any state $s \in \mathcal{S}$.
   \item Considering equations \ref{behaviourPolicy} and \ref{targetPolicy}, we have that
   \begin{equation} \label{policyQuotient}
        \frac{\pi_\theta (a|s)}{\mu(a|s)} = 
        \left\{ 
            \begin{array}{lcc}
                |\mathcal{A}|(1-\epsilon)+\epsilon  &   \text{if} & a = \text{arg max}_a Q_\theta(s,a) \\
                \epsilon  &  \text{otherwise} &
            \end{array}
        \right. 
    \end{equation}
\end{itemize}


\subsection{Exponential Bins}

\subsection{Log-Categorical COP-TD}


\newpage 

\thispagestyle{plain}
\section{Evaluation of the proposal}



\newpage

\thispagestyle{plain}
\section{Conclusions}


\newpage

\appendix
\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}
\renewcommand{\subsectionmark}[1]{\markright{\thesubsection.\quad #1}}

\thispagestyle{plain}
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}
\sectionmark{Appendix}

\setcounter{section}{1}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\subsection{Useful Distributional Notation}
\begin{remark}
All random variables presented in this document are considered to be real-valued, i.e. their measurable space is $E= \R$. 
\end{remark}

\subsubsection*{Mixture Distributions}

A random variable $Y$ is a mixture distribution if it is derived from a collection of other random variables $\{X_i\}$, $i\in \{1, \dots, N\}$, (named mixture components) in such a way that the combination of these parent distributions is driven according to a certain distribution $A$ (called mixing distribution). $A$ encapsulates the mixture weights $\alpha_i \sim A$, $i\in \{1, \dots, N\}$, which represent the probabilities of each individual mixture component $X_i$. 

The mixture distribution $Y$ can be defined in terms of its density function $f_Y$, which is the resulting $\alpha$-convex combination of the mixture components' density functions:
\begin{equation} \label{mixtureDef}
    f_{Y} (x) = \sum_{i=1}^{N} \alpha _i f_{X_i}(x)
\end{equation}

Let's present some interesting properties of mixture distributions:

\begin{mixtureProperty} \label{mixtureProperty1}
	The expectation of the mixture distribution $Y$ is the convex combination of expectations of each mixture component: 
    \begin{equation} \label{mixtureProperty1eq}
    \begin{split}
        \E [Y] &= \int_{-\infty}^{\infty} x f_{Y}(x) dx = \int_{-\infty}^{\infty} x \sum_{i=1}^{N} \alpha_i f_{X_i}(x) dx \\
        &= \sum_{i=1}^{N} \alpha_i \int_{-\infty}^{\infty} x f_{X_i}(x) dx \\
        &= \sum_{i=1}^{N} \alpha_i \E [X_i]
    \end{split}
    \end{equation}
\end{mixtureProperty}


\begin{mixtureProperty} \label{mixtureProperty2}
	Let be $Z$ a mixture distribution with mixture components $\{g_i(X_i)\}$, $i\in \{1, \dots, N\}$ and mixing weights $\alpha_i \sim A$
    \begin{equation} \label{mixtureProperty2eq}
        \E [Z] = \sum_{i=1}^{N} \alpha_i \E [g_i(X_i)]
    \end{equation}
\end{mixtureProperty}


\begin{mixtureProperty} \label{mixtureProperty3}
	Let be $Z=g(Y)$, being $Y$ a mixture distribution with mixture components $\{X_i\}$, $i\in \{1, \dots, N\}$, and $g$ a monotonic, invertible and differentiable function. Then we have that $Z$ is a mixture distribution whose expectation is
    \begin{equation} \label{mixtureProperty3eq}
        \begin{split}
        \E [Z] &= \int_{-\infty}^{\infty} g(x) f_{Y}(x) dx \\
        &= \int_{-\infty}^{\infty} g(x) \left( \sum_{i=1}^{N} \alpha_i f_{X_i}(x)\right) dx = \sum_{i=1}^{N} \alpha_i \int_{-\infty}^{\infty} g(x) f_{X_i}(x) dx \\
        &= \sum_{i=1}^{N} \alpha_i \E [g(X_i)]
        \end{split}
    \end{equation}
    Note that in both the first and last steps the so-called Law of the Unconscious Statistician has been applied, which states that
    \begin{equation} \label{LOTUS}
    	\E [g(X)] = \int_{-\infty}^{\infty} y f_{g(X)}(y) dy  = \int_{-\infty}^{\infty} g(x) f_{X}(x) dx
    \end{equation}

\end{mixtureProperty}

We emphasize the relevance of \textbf{Property \ref{mixtureProperty3}}. In distributional TD learning, the distribution mixture plays the role of the expectation in expected TD. But while $\E[g(x)] \not= g(\E[x])$, we can interchange mixtures and functions. This allows us to circumvent Jensen's inequalities.


\subsubsection*{Sum of Distributions} 

The sum of two independent random variables $X_1$ and $X_2$, $Y=X_1 + X_2$, results in a random variable whose density function is the convolution of the density functions of each summand
\begin{equation}
	f_{Y}(y) = \int_{-\infty}^{\infty} f_{X_1}(y-x) f_{X_2}(x)dx = \left(f_{X_1}* f_{X_2} \right) (y)
\end{equation}

In the general case, considering a collection $\{X_i\}$, $i\in \{1, \dots, N\}$, of independent random variables, the density function of the random variable $Y = \sum_{i=1}^{N} X_i $
can be expressed as the convolution of all the individual density functions:
\begin{equation}
	f_{Y} (x) = \left(f_{X_1} * \cdots * f_{X_N} \right) (x)
\end{equation}

Convolutions are LINEAR operators.


\subsection{Implementation Details}

TODO!!!

Our baseline is the C51 distributional reinforcement learning agent\cite{DRL} within Dopamine framework\cite{dopamine}. We use published hyperparameters unless otherwise noted. We augment the C51 network by adding an extra head, the distributional ratio model $X(s)$, to the final convolutional layer, whose role is to predict the distribution of the ratio $d_\pi / d_\mu$ . This model consists of a two-layer fully-connected network, with as many outputs as the number of atoms $M$ of the parametric model. A final softmax layer transforms the resulting logits into probabilities. 

\newpage

\thispagestyle{plain}
\addcontentsline{toc}{section}{Referencias}\sectionmark{Referencias}
\begin{thebibliography}{50}

	\bibitem{DRL} M. G. Bellamare, W. Dabney and R. Munos. A Distributional Perspective on Reinforcement Learning.
	\bibitem{COPTD} A. Hallak and S. Mannor. Consistent On-Line Off-Policy Evaluation.
	\bibitem{DCOPTD} C. Gelada and M. G. Bellamare. Off-Policy Deep Reinforcement Learning by Bootstrapping the Covariate Shift.
	\bibitem{unichain}  S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. 2012.
	\bibitem{TD} Sutton, R. S., and Barto, A. G. 2018. Reinforcement learning: An introduction. MIT Press, 2nd edition.
	\bibitem{projectedBellman} Tsitsiklis, J. N., and Van Roy, B. 1997. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control 42(5):674–690.
	\bibitem{dopamine} Dopamine: {A} Research Framework for Deep Reinforcement Learning. \url{https://github.com/google/dopamine}
	\bibitem{rlformulation} Bertsekas, D. and Tsitsiklis, J. Neuro-Dynamic Programming. Athena Scientific, 1996.
	\bibitem{ETD} Sutton, R. S., Mahmood, A. R., and White, M. An emphatic approach to the problem of off-policy temporal- difference learning. arXiv:1503.04269, 2015.
	\bibitem{generalizedETD} Hallak, Assaf, Tamar, Aviv, Munos, Remi, and Mannor, Shie. Generalized emphatic temporal differ- ence learning: Bias-variance analysis. arXiv preprint arXiv:1509.05172, 2015.






\end{thebibliography}


%\thispagestyle{plain}\nocite{*}\addcontentsline{toc}{section}{Referencias}\sectionmark{Referencias}\bibliography{Bibliografia} \bibliographystyle{unsrt}

\end{document}
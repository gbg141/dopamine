\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{verbatim} 
\usepackage[algoruled,noline,longend]{algorithm2e}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\R}{\mathbb{R}}
\DeclareMathOperator*{\N}{\mathbb{N}}
\DeclareMathOperator*{\Sspace}{\mathcal{S}}
\DeclareMathOperator*{\A}{\mathcal{A}}
\DeclareMathOperator*{\Y}{\hat{\mathcal{Y}}_{\Hat{\gamma}}}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}

\theoremstyle{plain}
\newtheorem{mixtureProperty}{Property}

\title{Distributional Covariate Shift Ratio}
\author{Carles Gelada, Sergio Escalera and Guillermo Bern√°rdez}

\begin{document}
\maketitle

%\begin{abstract}
%Your abstract.
%\end{abstract}

\section*{Motivation of Distributional Settings in RL}


As we will detail in the following sections, a distributional point of view of the reinforcement learning setting, such as the one presented in \cite{DRL}, allow us to express some random variables of the underlying Markov Decision Process by means of mixture distributions.

In this precise context, we are particularly interested in the potential implications of some interesting properties that mixture distributions satisfy. A clear example is that, in contrast to the classical value-based setting, where in the general case $\E [g(X)] \neq g(\E[X])$ by Jensen's inequality, this is somehow overcome when dealing with processes involving mixture distributions; see the next section for further details (specially \textbf{Property \ref{mixtureProperty3}} of Mixture Distributions, equation \ref{mixtureProperty3eq}).

In fact, the aforementioned property motivates the study of logarithmic approaches to particular distributional RL settings where a mixture distribution is intended to be learnt from a multiplicative updating rule or equation. The last section of the document, which is about the Covariate Shift Ratio learning process, faces this matter.


\section*{Useful Distributional Notation}

\begin{remark}
All random variables presented in this document are considered to be real-valued, i.e. their measurable space is $E= \R$. 
\end{remark}

\subsection*{Mixture Distributions}

A random variable $Y$ is a mixture distribution if it is derived from a collection of other random variables $\{X_i\}$, $i\in \{1, \dots, N\}$, (named mixture components) in such a way that the combination of these parent distributions is driven according to a certain distribution $A$ (called mixing distribution). $A$ encapsulates the mixture weights $\alpha_i \sim A$, $i\in \{1, \dots, N\}$, which represent the probabilities of each individual mixture component $X_i$. 

The mixture distribution $Y$ can be defined in terms of its density function $f_Y$, which is the resulting $\alpha$-convex combination of the mixture components' density functions:
\begin{equation} \label{mixtureDef}
    f_{Y} (x) = \sum_{i=1}^{N} \alpha _i f_{X_i}(x)
\end{equation}

Let's present some interesting properties of mixture distributions:

\begin{mixtureProperty} \label{mixtureProperty1}
	The expectation of the mixture distribution $Y$ is the convex combination of expectations of each mixture component: 
    \begin{equation} \label{mixtureProperty1eq}
    \begin{split}
        \E [Y] &= \int_{-\infty}^{\infty} x f_{Y}(x) dx = \int_{-\infty}^{\infty} x \sum_{i=1}^{N} \alpha_i f_{X_i}(x) dx \\
        &= \sum_{i=1}^{N} \alpha_i \int_{-\infty}^{\infty} x f_{X_i}(x) dx \\
        &= \sum_{i=1}^{N} \alpha_i \E [X_i]
    \end{split}
    \end{equation}
\end{mixtureProperty}


\begin{mixtureProperty} \label{mixtureProperty2}
	Let be $Z$ a mixture distribution with mixture components $\{g_i(X_i)\}$, $i\in \{1, \dots, N\}$ and mixing weights $\alpha_i \sim A$
    \begin{equation} \label{mixtureProperty2eq}
        \E [Z] = \sum_{i=1}^{N} \alpha_i \E [g_i(X_i)]
    \end{equation}
\end{mixtureProperty}


\begin{mixtureProperty} \label{mixtureProperty3}
	Let be $Z=g(Y)$, being $Y$ a mixture distribution with mixture components $\{X_i\}$, $i\in \{1, \dots, N\}$, and $g$ a monotonic, invertible and differentiable function. Then we have that $Z$ is a mixture distribution whose expectation is
    \begin{equation} \label{mixtureProperty3eq}
        \begin{split}
        \E [Z] &= \int_{-\infty}^{\infty} g(x) f_{Y}(x) dx \\
        &= \int_{-\infty}^{\infty} g(x) \left( \sum_{i=1}^{N} \alpha_i f_{X_i}(x)\right) dx = \sum_{i=1}^{N} \alpha_i \int_{-\infty}^{\infty} g(x) f_{X_i}(x) dx \\
        &= \sum_{i=1}^{N} \alpha_i \E [g(X_i)]
        \end{split}
    \end{equation}
    Note that in both the first and last steps the so-called Law of the Unconscious Statistician has been applied, which states that
    \begin{equation} \label{LOTUS}
    	\E [g(X)] = \int_{-\infty}^{\infty} y f_{g(X)}(y) dy  = \int_{-\infty}^{\infty} g(x) f_{X}(x) dx
    \end{equation}

\end{mixtureProperty}

Again, we emphasize the relevance of \textbf{Property \ref{mixtureProperty3}} in our motivation to study distributional RL settings, as equation \ref{mixtureProperty3eq} substitutes Jensen's inequalities in these cases.


\subsection*{Sum of Distributions} 

The sum of two independent random variables $X_1$ and $X_2$, $Y=X_1 + X_2$, results in a random variable whose density function is the convolution of the density functions of each summand
\begin{equation}
	f_{Y}(y) = \int_{-\infty}^{\infty} f_{X_1}(y-x) f_{X_2}(x)dx = \left(f_{X_1}* f_{X_2} \right) (y)
\end{equation}

In the general case, considering a collection $\{X_i\}$, $i\in \{1, \dots, N\}$, of independent random variables, the density function of the random variable $Y = \sum_{i=1}^{N} X_i $
can be expressed as the convolution of all the individual density functions:
\begin{equation}
	f_{Y} (x) = \left(f_{X_1} * \cdots * f_{X_N} \right) (x)
\end{equation}

Convolutions are LINEAR operators.


\newpage

\section*{Reinforcement Learning: Q-learning}

\subsection*{General Setting}

We consider an agent interacting with an environment in the standard setting: at each step $t$, the agent selects an action $a_t$ based on its current state $s_t$, to which the environment responds with a reward $r_t$ and then moves to the next state $s_{t+1}$. We model this interaction as a time-homogeneous Markov Decision Process $(\Sspace, \A, r, P, \gamma)$, where
\begin{itemize}
    \item $\Sspace$ and $\A$ are the state and action spaces, respectively, we assume that both are finite, with $n:=|\Sspace|$;
    \item $P$ is the transition kernel, $s_{t+1} \sim P(\cdot | s_t,a_t)$; the Markov assumption states that \\ $P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)$;
    \item $r(s,a)$ represents the immediate reward given by the environment after taking action $a$ being in state $s$. These rewards are considered to be sampled from the reward function $R(s,a)$, i.e. $r_t \sim R(s_t,a_t)$;
    \item $\gamma$ is the discount factor
\end{itemize}
A policy $\pi$ maps each state to a probability distribution over the action space, $a_t \sim \pi(\cdot | s_t)$. In addition, we combine the policy $\pi$ and transition function $P$ into a state-to-state transition function $P_\pi \in \R^{n \times n}$ , whose entries are
\begin{equation} \label{transitionFunction}
    P_{\pi} (s'|s) := Prob_\pi (s_{t+1}=s' | s_t = s) = \sum_{a\in\A} \pi(a|s)P(s'|s,a)
\end{equation}
In particular, powers of $P_\pi$ represent the transition function across different time-steps.

Given $\pi$, the action value function is defined as the expected sum of discounted rewards from a state-action pair by following the policy:
\begin{equation} \label{AV}
Q^\pi (s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t,a_t) \bigg| s_0=s, a_0=a \right] 
\end{equation}
The Bellman's equation can be obtained from this expression:
\begin{equation} \label{BE}
\begin{split}
    Q^\pi (s,a) &= \mathbb{E} \left[ r(s,a) \right] + \gamma \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_0=s, a_0=a \right] \\
    &= \mathbb{E} \left[ r(s,a) \right] + \gamma \sum_{s'} P(s'|s,a) \left( \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_1=s' \right] \right) \\
    &= \mathbb{E} \left[ r(s,a) \right] + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s')\left( \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_1=s', a_1=a' \right] \right)\\
    %&= \mathbb{E} \left[ r(s,a) \right] + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s') Q(s',a') \\
    &= \mathbb{E} \left[ r(s,a) \right] + \gamma \E_{s'\sim P(\cdot | s,a), a' \sim \pi(\cdot | s')} \bigg[Q(s',a') \bigg] \\
\end{split}
\end{equation}

Analogously for the state value function (considering $r_\pi(s) := \mathbb{E}_{a\sim\pi(\cdot | s)} \left[ r(s,a) \right]$):
\begin{equation} \label{SV}
\begin{split}
    V^\pi (s) &= \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t,a_t) \bigg| s_0=s\right] \\
    &= \mathbb{E}_{a\sim\pi(\cdot | s)} \left[ r(s,a) \right] + \gamma \sum_a \pi(a | s) \sum_{s'} P(s'|s,a) \left( \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_{t+1},a_{t+1}) \bigg| s_1=s' \right] \right) \\
    &= r_\pi(s) + \gamma \E_{s'\sim P_{\pi}(\cdot | s)} \bigg[V^\pi(s') \bigg]
\end{split}
\end{equation}


\newpage


\subsection*{Notation Analysis of Distributional RL}

In \cite{DRL} a distributional Bellman equation is defined:
\begin{equation} \label{distBE}
    Z^{\pi}(s,a) \stackrel{D}{=} R(s,a) + \gamma Z^{\pi}(S',A')
\end{equation}
where
\begin{itemize}
    \item $Z^\pi$ is the \textit{random return} from a state-action pair by following policy $\pi$, whose expectation is the value $Q^\pi$
    	\begin{equation} \label{QexpZ}
        		Q^\pi(s,a) := \mathbb{E} [Z^\pi(s,a)]
    	\end{equation}
    	It is also called the value distribution.
    \item $(S',A')$ is the next state-action random variable: $s'\sim S'$ with  $P(s' | s,a)$, $A' \sim \pi(\cdot | S')$
    \item $R(s,a)$ is the random reward, or equivalently the reward function. Note that now we are dealing with it as an explicit random variable.
    \item $Z^\pi(S',A')$ is the random return over the random next state-action following $\pi$. This notation implies that all possible next state-action pairs need to be considered as to generate this return distribution. Thus, $Z^\pi(S',A')$ may be seen as a mixture distribution of the distributions $Z^\pi(s',a')$ where $s'$ and $a'$ are sampled from $(S',A')$:
    \begin{equation}
        f_{Z^\pi(S',A')} (z) = \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{Z^\pi(s',a')}(z)
    \end{equation}
    The expected value, using \ref{QexpZ}, can be then expressed as:
    \begin{equation} \label{expNextRandomReturn}
    \begin{split}
        \E[Z^\pi(S',A')] &=  \int_{-\infty}^{\infty} z \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{Z^\pi(s',a')}(z) dz \\
        &=  \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') \int_{-\infty}^{\infty} z f_{Z^\pi(s',a')}(z) dz \\
        &=  \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') \E[Z^\pi(s',a')] \\
        &= \E_{s'\sim P(\cdot | s,a), a' \sim \pi(\cdot | s')} \bigg[Q^\pi(s',a') \bigg] 
    \end{split}
    \end{equation}
\end{itemize}

Note that we can easily recover the classical Bellman's equation \ref{BE} for the action value Q by using \ref{QexpZ} and \ref{expNextRandomReturn} when taking the expected value over its distributional version \ref{distBE}:
\begin{equation}
\begin{split}
    Q^\pi(s,a) &= \mathbb{E} [Z^\pi(s,a)] \\
    &= \E[R(s,a)] + \gamma \E[Z^\pi(S',A')] \\
    &= \mathbb{E} [r(s,a)] + \gamma \E_{s'\sim P(\cdot | s,a), a' \sim \pi(\cdot | s')} \bigg[Q^\pi(s',a') \bigg] 
\end{split}
\end{equation}

Finally, let's try to find out what actually the random return $Z$ represents by expanding its density function: 
\begin{equation} \label{meaningZ}
\begin{split}
    f_{Z^\pi(s,a)}(z) &= f_{R(s,a) + \gamma Z^\pi(S',A')}(z)  \\
    &=  \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{R(s,a) + \gamma Z^\pi(s',a')}(z) \\
    &= \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') f_{R(s,a) + \gamma (R(s',a') + \gamma Z^\pi(S'',A''))}(z) \\
    &= \sum_{s'} P(s' | s,a) \sum_{a'} \pi(a'|s') \sum_{s''} P(s'' | s',a') \sum_{a''} \pi(a''|s'') \\
    & \quad \quad f_{R(s,a) + \gamma R(s',a') + \gamma^2 Z^\pi(s'',a'')} (z) \\
    &= \sum_{s_{1}} P(s_{1} | s_{0},a_{0}) \sum_{a_{1}} \pi(a_{1}|s_{1})  \cdots \sum_{s_{t}} P(s_{t} | s_{t-1},a_{t-1}) \sum_{a_{t}} \pi(a_{t}|s_{t}) \\
    & \quad \quad f_{\sum_{i=0}^{t} \gamma^t R(s_{t},a_{t})} (z)
\end{split}
\end{equation}
where we have repeatedly used property \ref{mixtureProperty2} of mixture distributions. In addition, note that assuming independence between the random reward $R$ of a certain state-action pair and the return distribution $Z^\pi$ of the possible next state-action (which is NOT TRUE in general), we can rewrite it in terms of convolutions as
\begin{equation}
\begin{split}
    f_{Z^\pi(s,a)}(z) &= \sum_{s_{1}} P(s_{1} | s_{0},a_{0}) \sum_{a_{1}} \pi(a_{1}|s_{1})  \cdots \sum_{s_{t}} P(s_{t} | s_{t-1},a_{t-1}) \sum_{a_{t}} \pi(a_{t}|s_{t}) \\
    & \quad \quad  \left( f_{R(s_0,a_0)} * f_{\gamma R(s_1,a_1)} *  \cdots * f_{\gamma^t R(s_{t},a_{t})} \right) (z)
\end{split}
\end{equation}

According to \ref{meaningZ}, $Z(s,a)$ can be interpreted as a convex combination of the sum of discounted reward distributions of all possible agent trajectories starting at the state-action $(a,s)$ and following policy $\pi$ from then on, each weight corresponding to the probability of that precise trajectory. 

\newpage

\section*{Covariate Shift Ratio}

\subsection*{General Setting}

As stated in \cite{DCOPTD}, here we move to the \textit{policy evaluation} problem within \textit{off-policy learning}, where we want to learn the value function $V^\pi$ of a \textit{target policy} $\pi$ from samples drawn from $P$ and a \textit{behaviour policy} $\mu$. Some useful notation:
\begin{itemize}
    \item The Bellman equation for the state value function can be expressed in vector notation as $V^\pi = r_\pi + \gamma P_\pi V^\pi$, where $V^\pi \in \R^{n}$, $r_\pi \in \R^n$ and $P_\pi \in \R^{n \times n}$. The value function is in fact the fixed point of the \textit{Bellman operator} $\mathcal{T}_\pi: \R^{n \times n} \rightarrow \R^{n \times n}$, defined as $\mathcal{T}_\pi V := r_\pi + \gamma P_\pi V $. It defines a single step of \textit{bootstrapping}: the process $V^{k+1} := \mathcal{T}_\pi V^k$ converges to $V^\pi$.
    \item Let $d \in \R^n$; we write $D_d \in \R^{n\times n}$ for the corresponding diagonal matrix, and consider the weighted squared seminorm notation of vectors $x\in\R^n$ $||x||^2_{A} := ||A x||^2 = x^T A^T A x$, $||x||^2_d :=||x||^2_{D_d}= \sum_{i=1}^n d(i)^2 x(i)^2$.
    \item $e\in\R^n$ accounts for the vector of all ones, and $\Delta(\Sspace)$ for the simplex over states: $d \in \Delta(\Sspace) \implies d^T e = 1, d \geq 0$.
    \item $d \in \Delta(\Sspace)$ is the stationary distribution of a transition function $P$ if and only if $d=d \cdot P$. This distribution is unique when $P$ defines a Markov chain with a single recurrent class\cite{unichain}.
\end{itemize}
In this particular setting we distinguish between two different state-to-state transition functions, $P_\pi$ and $P_\mu$, one for each policy; their respective stationary distributions will be represented by $d_\pi$ and $d_\mu$. 

It is common to estimate the value function through \textit{linear function approximation}, which uses a mapping from states to features $\phi: \Sspace \rightarrow \R^k$. In these cases, the approximate value function at a state $s$ can be expressed as the inner product of a feature vector with a vector of weights $\theta \in \R^k$:
\begin{equation} \label{linearApproxSV}
    \Hat{V}(s) = \phi(s)^T \theta
\end{equation}
which can be written as $\Hat{V} = \Phi \theta$ in vector notation, being $\Phi \in \R^{n\times k}$ the matrix of row vectors. The \textit{semi-gradient update rule} for TD learning\cite{TD} learns an estimation of $V^\pi$ from sample transitions. Given a starting state $s\in\Sspace$, a successor state $s'\sim P_\pi(\cdot | s)$, and a step-size parameter $\alpha > 0$, this update is
\begin{equation} \label{updateTD}
    \theta \leftarrow \theta + \alpha \left[ r_\pi(s) + \gamma\phi(s')^T \theta - \phi(s)^T \theta \right] \phi(s)
\end{equation}

The expected behaviour of this update rule is described by the \textit{projected Bellman operator} $\Pi_d \mathcal{T}_\pi$, a combination of the usual Bellman operator with a projection $\Pi_d$ in norm $||\cdot||_d$ -for some $d\in\Delta(\Sspace)$- onto the span of $\Phi$\cite{projectedBellman}. In fact, the stationary point of \ref{updateTD}, if it exists, is the solution of the \textit{projected Bellman equation} $\Hat{V}^\pi = \Pi_d \mathcal{T}_\pi \Hat{V}^\pi$. As stated in \cite{DCOPTD}, not only convergence is proved for $d=d_\pi$, but this choice also seems optimal in terms of the quality of the fixed point under off-policy data.

Supposing that stationary distributions $d_\pi$ and $d_\mu$ are known, and that states are updated according to $s\sim d_\mu$, the covariate shift approach presented in \cite{COPTD} uses importance sampling to redefine \ref{updateTD} so that the semi-gradient update rule can be considered \textit{under the sampling distribution} $d_\pi$:
\begin{equation} \label{updateTDimportance}
    \theta \leftarrow \theta + \alpha \frac{d_\pi(s)}{d_\mu (s)}\left[ r(s,a) + \gamma\phi(s')^T \theta - \phi(s)^T \theta \right] \phi(s)
\end{equation}
with $a\sim \mu(\cdot|s)$, $s' \sim P(\cdot|s,a)$ as before. 

Both the original COP-TD learning rule\cite{COPTD} and its discounted version\cite{DCOPTD} seek to learn the ratio $d_\pi / d_\mu$ from samples -by bootstrapping from a previous prediction, similar to temporal difference learning. For instance, given a sample transition $(s_t,a_t,s_{t+1}) = (s,a,s')$ drawn from $d_\mu$, $\mu(\cdot|s)$ and $P(\cdot|s,a)$, respectively, and for $c\in \mathbb{R}^n$, step size $\alpha>0$ and discount factor $\hat{\gamma} \in [0,1]$, the Discounted COP-TD provide us with the following update
\begin{equation} \label{DCOP}
    c(s') \leftarrow c(s') + \alpha \left[\hat{\gamma} \frac{\pi(a|s)}{\mu(a|s)}c(s) + (1-\hat{\gamma}) - c(s') \right]
\end{equation}
This update rule learns ‚Äúin reverse‚Äù compared to TD learning, its expected behaviour being captured by the operator $Y_{\Hat{\gamma}}$
\begin{equation} \label{DCOPoperator}
    (Y_{\Hat{\gamma}}c)(s') := \E_{s \sim d_\mu, a \sim \mu(\cdot|s)} \left[ \hat{\gamma} \frac{\pi(a|s)}{\mu(a|s)}c(s) + (1-\hat{\gamma}) \bigg| s' \right] = \hat{\gamma} (Yc)(s') + (1-\hat{\gamma})
\end{equation}
where $Y$ is the original COP operator 
\begin{equation} \label{COPoperator}
    (Yc)(s') := \E_{s \sim d_\mu, a \sim \mu(\cdot|s)} \left[ \frac{\pi(a|s)}{\mu(a|s)}c(s)  \bigg| s' \right]
\end{equation}
which corresponds to the undiscounted case, $Y_1=Y$.

Note that the condition $s_{t+1} = s'$ in the expectation of \ref{COPoperator} forces to take into account the distribution of previous state-action pairs $(s,a)$ according to policy $\mu$. The distribution of the possible previous states $s$ is given by the time-reversal transition function $\Bar{P}_\mu$, whose entries are:
\begin{equation}
\begin{split}
    \Bar{P}_\mu (s | s') &:= Prob_\mu (s_t = s | s_{t+1} = s') \\
    &= \frac{Prob_\mu ( s_{t+1} = s' | s_t = s) Prob_\mu(s_t=s)}{Prob_\mu ( s_{t+1} = s')} \\
    &= \frac{P_\mu (s' | s) d_\mu(s)}{d_\mu(s')} 
\end{split}
\end{equation}
Or, equivalently, $ \Bar{P}_\mu = D_{d_\mu}^{-1}P_\mu^T D_{d_\mu}$ in vector notation. Regarding the distribution of the possible actions that lead to $s'$ from a certain state $s$ by following policy $\mu$, it will be represented by function $\Bar{\mu}$:  
\begin{equation}
\begin{split}
        \Bar{\mu}(a|s,s') &:= Prob_\mu(a_t=a | s_t = s, s_{t+1}=s') \\
        &= \frac{Prob_\mu (a_t=a, s_t = s, s_{t+1}=s')}{Prob_\mu (s_t = s, s_{t+1}=s')} \\
        %&= \frac{Prob_\mu (s_{t+1}=s' | a_t=a, s_t = s) Prob_\mu(a_t=a, s_t = s)}{Prob_\mu (s_t = s, s_{t+1}=s')} \\
        &= \frac{Prob_\mu (s_{t+1}=s' | a_t=a, s_t = s) Prob_\mu (a_t=a | s_t = s) Prob_\mu (s_t = s)}{Prob_\mu (s_{t+1}=s' | s_t = s) Prob_\mu (s_t = s)} \\
        &= \frac{P(s'|s,a)\mu(s|a)}{P_\mu (s'|s)}
\end{split}
\end{equation}


With the introduced notation, the expectation in \ref{COPoperator} can be rewritten and expanded in the following way:
\begin{equation} \label{expansionCSR}
\begin{split}
    \E_{s \sim d_\mu, a \sim \mu(\cdot|s)} \left[ \frac{\pi(a|s)}{\mu(a|s)}c(s)  \bigg| s' \right] &= \E_{s \sim \Bar{P}_\mu (\cdot | s'), a \sim \Bar{\mu}(\cdot|s,s')} \left[ \frac{\pi(a|s)}{\mu(a|s)}c(s) \right] \\
    &= \sum_s \Bar{P}_\mu (s | s') \sum_a \Bar{\mu}(a|s,s') \frac{\pi(a|s)}{\mu(a|s)} c(s) \\
    &= \sum_s \left( \frac{P_\mu (s' | s) d_\mu(s)}{d_\mu(s')}  \right) \sum_a  \left( \frac{P(s'|s,a)\mu(s|a)}{P_\mu (s'|s)} \right) \frac{\pi(a|s)}{\mu(a|s)} c(s) \\
    &= \frac{1}{d_\mu(s')} \sum_{s} d_\mu(s) c(s) \sum_a \pi(a|s) P(s'|s,a) \\
    &= \frac{1}{d_\mu(s')} \sum_{s} P_\pi(s'|s) d_\mu(s) c(s) \\
    %&=  \frac{1}{d_\mu(s')} \E_{s\sim d_\mu} \left[P_\pi(s'|s) c(s)\right]
\end{split}
\end{equation}
Thus, the COP and DCOP operators can be expressed in vector notation, respectively, as
\begin{equation}
    Yc = D_{d_\mu}^{-1}P_\pi^T D_{d_\mu} c
\end{equation}

\begin{equation}
    Y_{\Hat{\gamma}}c = \Hat{\gamma} D_{d_\mu}^{-1}P_\pi^T D_{d_\mu} c + (1-\Hat{\gamma})e
\end{equation}

\newpage

\subsection*{Moving towards a Distributional Covariate Shift Ratio}

Now we are interested in going beyond the notion of value and consider the estimation of the CS ratio from a distributional perspective, similarly to what was done in \cite{DRL} within the reinforcement learning setting. Our starting point could be 
\begin{equation} \label{distCSR}
    X(s') \stackrel{D}{=} \frac{\pi(A^\mu_{s,s'}|S^\mu_{s'})}{\mu(A^\mu_{s,s'}|S^\mu_{s'})} X(S^\mu_{s'}) 
    %X(s') \stackrel{D}{=} \hat{\gamma} \frac{\pi(A^\mu_{s,s'}|S^\mu_{s'})}{\mu(A^\mu_{s,s'}|S^\mu_{s'})} X(S^\mu_{s'}) +1 -\hat{\gamma}
\end{equation}
where
\begin{itemize}
    \item $X$ is the random ratio between distributions of achieving a certain state, its expectation being the covariate shift ratio 
    \begin{equation}
        \frac{d_\pi}{d_\mu}(s) = c(s) = \E [X(s)]
    \end{equation}
    
    \item $(S^\mu_{s'},A^\mu_{s,s'} )$ is the previous state-action random variable:
   	\begin{itemize}
    	\item The random variable $S^\mu_{s'}$ represents the states from which state $s'$ is achievable by following policy $\mu$; $S^\mu_{s'} =s$ with probability $\Bar{P}_\mu(s|s')$
   	 	\item $A^\mu_{s,s'}$ encodes the random action that can be taken to get state $s'$ from a state $s\sim S^\mu_{s'}$ according to policy $\mu$, so $A^\mu_{s,s'} =a$ with probability $\Bar{\mu}(a|S^\mu_{s'},s')$
   	 \end{itemize}
 
\end{itemize}

Recalling equation \ref{distCSR}, note that it intrinsically expresses the random ratio of a state $s_{t+1}$, $X(s_{t+1})$, as a mixture distribution with the 'corrected' previous state random ratios $\rho(s_{t},a_{t}) X(s_{t})$ as mixing components, and the previous state-action random variable $(S^\mu_{s_{t+1}},A^\mu_{s_{t},s_{t+1}} )$ as the mixing distribution:
\begin{equation} \label{densityDCS}
    f_{X(s_{t+1})}(x) = \sum_{s_{t}} \Bar{P}_\mu (s_{t} | s_{t+1}) \sum_{a_t} \Bar{\mu}(a_t|s_{t},s_{t+1}) f_{\frac{\pi(s_{t},a_{t})}{\mu(s_{t},a_{t})} X(s_{t})}(x) 
\end{equation}

\begin{notation}
	So as to reduce the complexity and increase the readability of the formulation, we introduce the following notation:
	
	\begin{itemize}
		\item Let define $\rho$ the policy ratio $\pi / \mu$:
%\begin{equation}
%	\rho(s_{t},a_{t}) = \rho (A^\mu_{s_{t},s_{t+1}},S^\mu_{s_{t+1}}) := \frac{\pi(A^\mu_{s_{t},s_{t+1}}|S^\mu_{s_{t+1}})}{\mu(A^\mu_{s_{t},s_{t+1}}|S^\mu_{s_{t+1}})}
%\end{equation}
		\begin{equation*}
			\rho(a,s) := \frac{\pi(a|s)}{\mu(a|s)}
		\end{equation*}

	\item Note in equation \ref{densityDCS} that there are as many mixture components as state-action pairs $(s_t,a_t)$; thus, we can iterate the summation over all these possible pairs and define each corresponding mixture weight $\alpha$ as
		\begin{equation*}
			\alpha(s_{t},a_{t}; s_{t+1}) = \Bar{P}_\mu (s_{t} | s_{t+1}) \Bar{\mu}(a_t|s_{t},s_{t+1}) 
		\end{equation*}
	\end{itemize}
\end{notation}

Considering the previous notation, the expression of the density function \ref{densityDCS} can be rewritten and expanded in the following way:
\begin{equation}
\begin{split}
    f_{X(s_{t+1})}(x) &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\rho(s_{t},a_{t}) X(s_{t})}(x) \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\rho(s_{t},a_{t}) \rho(S^{\mu}_{s_{t}},A^{\mu}_{s_{t-1},s_{t}}) X(S^{\mu}_{s_t}) )}(x) \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \sum_{(s_{t-1},a_{t-1})} \alpha(s_{t-1},a_{t-1};s_{t})   \cdot f_{\rho(s_{t},a_{t}) \rho(s_{t-1},a_{t-1}) X(s_{t-1})}(x) \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdots \sum_{(s_{0},a_{0})} \alpha(s_{0},a_{0};s_{1}) \cdot f_{ \left(\prod_{i=t}^{0} \rho(s_{i},a_{i})\right)  X(s_{0})}(x)
\end{split}
\end{equation}

Regarding its expectation, we can see that:
\begin{equation}
\begin{split}
    \E[X (s_{t+1})] &= \int_{-\infty}^{\infty} x f_{X(s_{t+1})}(x) dx \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \int_{-\infty}^{\infty} x f_{\rho(s_{t},a_{t}) X(s_{t})}(x) dx \\
    &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \E \left[ \rho(s_{t},a_{t}) X(s_{t}) \right] \\
    &=  \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \rho(s_{t},a_{t}) \cdot \E \left[X(s_{t}) \right] 
\end{split}
\end{equation}

\subsubsection*{Discounted Distributional CS Ratio}
Having already develop the previous formulation, it is straightforward to turn equation \ref{distCSR} into its discounted version as it is done in \cite{DCOPTD} in the value-based case:
\begin{equation} \label{distDCSR}
    X(s_{t+1}) \stackrel{D}{=} \hat{\gamma} \ \rho(S^{\mu}_{s_{t+1}},A^{\mu}_{s_{t},s_{t+1}}) X(S^{\mu}_{s_{t+1}}) + 1 -\hat{\gamma}
\end{equation}
as well as compute its corresponding expectation:
\begin{equation}
\E[X (s_{t+1})] = 1- \hat{\gamma}\left(1 - \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \rho(s_{t},a_{t}) \cdot \E \left[X(s_{t}) \right]  \right)
\end{equation}

Given that $\E \left[X(s) \right] = c(s)$, the previous equation is equivalent to the result of applying of the DCOP operator, whose convergence is analyzed and proved in \cite{DCOPTD}. This can help us prove the convergence to a fixed point in the distributional setting, whose expectation matches with the covariate shift ratio estimate $c$ in the value-based case.


\newpage

\subsection*{Logarithmic Approach to Distributional COP-TD}

Note that the Distributional Covariate Shift equation is purely multiplicative, and so it is the associated update rule in the learning setting. 


Let's consider
\begin{equation}
	Y(s_{t+1}) := \log \left( X(s_{t+1}) \right) = \log \left( \rho(S^{\mu}_{s_{t+1}},A^{\mu}_{s_{t},s_{t+1}}) \right) + Y(S^{\mu}_{s_{t+1}}) 
\end{equation}
Its density function being
\begin{equation}
\begin{split}
	f_{Y(s_{t+1})} (x) &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\log\left( \rho(s_{t},a_{t}) X(s_{t}) \right)}(x) \\
	&= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot f_{\log\left( \rho(s_{t},a_{t})\right) + Y(s_t)}(x)
\end{split}
\end{equation}

Recalling \textbf{Property \ref{mixtureProperty3}} of Mixture Distributions, we can express the expectation of $Y$ as:
\begin{equation}
\begin{split}
	\E[Y(s_{t+1})] &= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \E \left[ \log \left( \rho(s_{t},a_{t}) X(s_{t}) \right) \right] \\
	&= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \E \left[ \log \left( \rho(s_{t},a_{t}) \right)  + Y(s_t) \right] \\
	&= \sum_{(s_{t},a_{t})} \alpha(s_{t},a_{t};s_{t+1}) \cdot \left( \log \left( \rho(s_{t},a_{t}) \right) + \E [Y(s_t)] \right)
\end{split}
\end{equation}

TODO: ADDITIVE FIXED POINT
\\

And if
\begin{equation}
	U(s') = \exp (Y(s'))
\end{equation}
expectation:
\begin{equation}
\begin{split}
	\E[U(s_{t+1})] &= \int_{-\infty}^{\infty} \exp(y) f_{Y(s_{t+1})} (y) dy \\
	&= \int_{-\infty}^{\infty} \exp(\log(x)) f_{X(s_{t+1})} (x) dx \\
	&=  \int_{-\infty}^{\infty} x f_{X(s_{t+1})} (x) dx  \\
	&= \E [X(s_{t+1})]
\end{split}
\end{equation}



\newpage


\subsection*{Approximate CS Distributional Learning}

Analogously to \cite{DRL}, we can attempt to model the CS ratio distribution through a discrete parametric distribution with a certain set of atoms $\{x_i\}_{0 \leq i<M}$, $M \in \mathbb{N}$, as its support. The atom probabilities would be given by a parametric model $\theta : \mathcal{X} \rightarrow \mathbb{R}^M$
\begin{equation}
    X_{\theta} (s) = x_i \quad w.p. \quad p_i(s) := f_{\theta}^i(s)
\end{equation}

The DCOP update $Y_{\hat{\gamma}}X_\theta$ and this parametrization $X_\theta$ almost always would have disjoint supports. This could be tackled in practice projecting the sample DCOP update $\hat{Y}_{\hat{\gamma}}X_\theta$ onto the support of $X_\theta$ (i.e. given a sample transition $(s,a,s')$, we compute the DCOP update $\hat{Y}_{\hat{\gamma}}x_j =  \frac{\pi(a|s)}{\mu(a|s)}x_j + (1-\hat{\gamma})$ for each atom $x_j$, then distribute its probability $p_j(s)$ to the immediate neighbours of $\hat{Y}_{\hat{\gamma}}x_j$).

The corresponding pseudocode is detailed in Algorithm \ref{distCSalgorithm}, analogous to that of \cite{DRL}:

\setlength{\algomargin}{1.5em}
\SetAlCapHSkip{0em}
\begin{algorithm}[H]
\caption{Categorical CS Algorithm} \label{distCSalgorithm}
\DontPrintSemicolon
\SetKwComment{CustomComment}{\#}{}
\SetKwInput{KwIn}{\hspace{-1.5em} input}
\SetKwInput{KwOut}{\hspace{-1.5em} output}
\KwIn{A transition $s_{t-1},a_{t-1},s_t$}
    %$\frac{d_\pi}{d_\mu}(s_{t-1}) := \sum_i x_i p_i(s_{t-1})$\;
    $m_i = 0$, $i\in \{0,\dots, M-1\}$\;
    \For{$j\in \{0,\dots, M-1\}$}{
        \CustomComment{Compute the projection $\tilde{T}x_j$ onto the support $\{x_i\}$}
        $\tilde{T}x_j \leftarrow \left[ \hat{\gamma} \frac{\pi(a_{t-1}|s_{t-1})}{\mu(a_{t-1}|s_{t-1})} x_j + 1 - \hat{\gamma} \right]_{C_{min}}^{C_{max}}$\;
        $b_j \leftarrow (\tilde{T}x_j -C_{min}) / \Delta x \quad$ \CustomComment{note that $b_j \in [0, M-1]$}
        $l \leftarrow \lfloor b_j \rfloor$, $u \leftarrow \lceil b_j \rceil$\;
        \CustomComment{Distribute probability of $\tilde{T}x_j$}
        $m_l \leftarrow m_l + p_j(s_{t-1}) (u - b_j)$\;
        $m_u \leftarrow m_u + p_j(s_{t-1}) (b_j - l)$\;
    }
\KwOut{Cross-entropy loss $-\sum_i m_i \log(p_i(s_t))$}

\end{algorithm}

Observations:
\begin{itemize}
    \item $C_{min}, C_{max} \in \R$ are the predefined lower and upper value limits of the covariate shift ratio. In this case, $C_{min} \geq 0$.
    \item The support is evenly spaced in $[C_{min}, C_{max}]$; we consider the set of atoms $\{x_i = C_{min} + i \Delta x : 0 \leq i < M \}$, with $\Delta x = \frac{C_{max}-C_{min}}{M-1}$
    \item The \textit{behavioural policy} $\mu$ is simply the uniformly random policy, so 
    \begin{equation} \label{behaviourPolicy}
        \mu(a|s) = \frac{1}{|\mathcal{A}|} \quad \forall a \in \mathcal{A}
    \end{equation}
    for any state $s \in \mathcal{S}$.
    \item The \textit{target policy} $\pi$ is the $\epsilon$-greedy policy with respect to the estimated state-action values of the model. Hence,%expected value of the distribution output of the target network. Hence,
    \begin{equation} \label{targetPolicy}
        \pi_\theta (a|s) = 
        \left\{ 
            \begin{array}{lcc}
                (1-\epsilon)+\epsilon \frac{1}{|\mathcal{A}|} &   \text{if} & a = \text{arg max}_a Q_\theta(s,a) \\
                \epsilon \frac{1}{|\mathcal{A}|} &  \text{otherwise} &
            \end{array}
        \right. 
    \end{equation}
   for any state $s \in \mathcal{S}$.
   \item Considering equations \ref{behaviourPolicy} and \ref{targetPolicy}, we have that
   \begin{equation} \label{policyQuotient}
        \frac{\pi_\theta (a|s)}{\mu(a|s)} = 
        \left\{ 
            \begin{array}{lcc}
                |\mathcal{A}|(1-\epsilon)+\epsilon  &   \text{if} & a = \text{arg max}_a Q_\theta(s,a) \\
                \epsilon  &  \text{otherwise} &
            \end{array}
        \right. 
    \end{equation}
\end{itemize}


\newpage

\subsection*{Implementation}

Our baseline is the C51 distributional reinforcement learning agent\cite{DRL} within Dopamine framework\cite{dopamine}. We use published hyperparameters unless otherwise noted. We augment the C51 network by adding an extra head, the distributional ratio model $X(s)$, to the final convolutional layer, whose role is to predict the distribution of the ratio $d_\pi / d_\mu$ . This model consists of a two-layer fully-connected network, with as many outputs as the number of atoms $M$ of the parametric model. A final softmax layer transforms the resulting logits into probabilities. 




\newpage



\begin{thebibliography}{25}
	\bibitem{DRL} M. G. Bellamare, W. Dabney and R. Munos. A Distributional Perspective on Reinforcement Learning.
	\bibitem{COPTD} A. Hallak and S. Mannor. Consistent On-Line Off-Policy Evaluation.
	\bibitem{DCOPTD} C. Gelada and M. G. Bellamare. Off-Policy Deep Reinforcement Learning by Bootstrapping the Covariate Shift.
	\bibitem{unichain}  S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. 2012.
	\bibitem{TD} Sutton, R. S., and Barto, A. G. 2018. Reinforcement learning: An introduction. MIT Press, 2nd edition.
	\bibitem{projectedBellman} Tsitsiklis, J. N., and Van Roy, B. 1997. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control 42(5):674‚Äì690.
	\bibitem{dopamine} Dopamine: {A} Research Framework for Deep Reinforcement Learning. \url{https://github.com/google/dopamine}
\end{thebibliography}

\end{document}